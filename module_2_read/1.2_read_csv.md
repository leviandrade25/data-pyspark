# Schema
When working with PySpark, defining a schema for your data is a crucial step in ensuring efficient and accurate processing. A schema essentially outlines the structure of your data, specifying the data types of each column. While PySpark provides an option to infer the schema automatically (inferSchema), there are several reasons why it's better to create the schema manually.

# Watch a video about [Schema](https://www.youtube.com/watch?v=7mfS5WMRy20)

```python

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql import functions as F

spark = SparkSession.builder\
  .config("spark.app.name", "sparksession")\
  .getOrCreate()

schema_file = "name STRING, surname STRING, age INT"
df = (spark.read
           .format("csv")
           .option("header","true")
           .option("sep","|")
           .schema(schema_file)
           .load("/home/levi/names.csv"))




df.show()